
# Machine Learning Project 2 : Multi-object detection and tracking

## Overview of files

* In the top-level directory, you will the different scripts that we wrote for this project :
    * `create_tfrecord.py`: A program that generates TFRecords from MOT Challenge data.
    * `create_config.py`: A program that modifies a pre-trained model's default pipeline.config, in order to adapt it to our own model.
    * `demo_ssd_mosse.py`: A demo program that runs SSD detection with MOSSE tracking on a given input video.
* The `tensorflow-models` package contains the code of the [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection).
* The `deep_sort` package contains the official implementation of DeepSORT tracking from [Nicolas Wolkje](https://github.com/nwojke/deep_sort) and modified by [pythonlessons](https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3/tree/master/model_data for Tensorflow V2 compatibility.
* The `images` folder contains train and test videos from [MOT Challenge]{https://motchallenge.net}. For each video the frames are located in the subfolder `img1` and the detections file `det.txt` in the subfolder `det`.
* The `results` folder is initially empty and is intended to contain the results from the `demo_ssd_mosse.py`, when the folder path is specified along with the flag -o.
* The `training` folder contains the data we need for training the model.
    * The folder `pre-trained-models` contains the pre-trained MobileNet SSD model. Others pre-trained models can be downloaded from the [Tensorflow Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)
    * The `tf-records` folder contains our single-entry label map, and is intended to contain the TFRecords generated by `create_tfrecord.py`.
    * The `trained-model`folder contains the pipeline.config created by `create_config.py`, along with the checkpoint files. Initially, we included a checkpoint-6 from a training that we did ourselves on Google Colab, but you are free to empty this whole folder and redo the entire training by following the [Training](#Training "Goto Training") section.
* `People.mp4` can be used for the demo.
* `Project-2-ML.ipynb` is the notebook you can run on Google Colab. All our work can be simply reproduced with this notebook.


## Installation :

If you are not running on macOS, some steps of the following configuration may fail. If you encounter problems or simply want to avoid all the configuration on your computer, you can simply run the attached notebook "Project_2_ML.ipynb" on Google Collaboratory and go through the steps.
Furthermore, Google Collaboratory allows to use powerful CPUs and GPUs, which makes the training and demos faster. If you run the notebook on Google Collab, in the menu, go in Edit > Notebook settings, and select GPU in the "Hardware accelerator" field.\

You will need to run the protocol buffer (protobuf) compiler protoc to generate some python files from the Tensorflow Object Detection API.
```bash
brew install protobuf
```

Create a new virtual environment with anaconda.
```bash
conda create -n tensorflow pip python=3.8
```

install tensorflow
```bash
pip install tensorflow
```

Install evrything else that is required by the object-detection API.
```bash
cd tensorflow_models/research
protoc object_detection/protos/*.proto --python_out=.
cp object_detection/packages/tf2/setup.py .
pip install .
pip install opencv-contrib-python -U
```

WARNING : cv2.MultiTracker_create() will be used for tracking, and is present only in package opencv-contrib-python (not in opencv-python, that may be installed as well by default).
If an error occur while calling MultiTracker_create(), uninstall opencv-python and opencv-contrib-python, and reinstall opencv-contrib-python only.
```bash
pip uninstall opencv-contrib-python
pip install opencv-contrib-python
```

## Training

We included in the repo a trained model (see ckpt-6 in the /training/trained-model directory). If you want to run the demo directly, you can skip this section and go to the next section "Demo".

If you want to run the training, please start by deleting every file under training/trained-models.

Our .py scripts are documented and a description of the arguments is present in the respective files.

First, we need to convert the train and test data provided by MOT challenge into TFRecords.
```bash
!python create_tfrecord.py images/train/ training/TFRecords/train.record training/TFRecords/label_map.pbtxt -f 10
!python create_tfrecord.py images/train/ training/TFRecords/test.record training/TFRecords/label_map.pbtxt -f 10
```

Then, we need to create a customized pipeline.config file for our specific traing. We will take the default config of the pre-trained model and modify it adequately. The batch size is 4 by default, you can set it to a higher value (e.g. 8 or 16) with flag -b if you are using Google Colab with GPUs.

```bash
!python create_config.py training/pre-trained-model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8 training/TFRecords/label_map.pbtxt training/TFRecords training/trained-model
```

Now we are ready to run the training. This script is provided by the Tensorflow object detection API. Depending on the number of steps that you specify (in the following it is set to 5000), and the batch size (specified in the pipeline configuration above), the trainig may take some time. The program prints the progression at every 100 steps.
```bash
!python tensorflow_models/research/object_detection/model_main_tf2.py --model_dir training/trained-model/ --pipeline_config_path training/trained-model/pipeline.config --num_train_steps 5000
```

## Demo

In order to test our model, simply run the following script. You have the choice to save the results with the -o flag and/or to visualize them in real time with the -d flag.

```bash
# saves the results in results/ directory :
!python demo_ssd_mosse.py People.mp4 training/trained-model/ -o results/

# display the images in real time.
!python demo_ssd_mosse.py People.mp4 training/trained-model/ -d
```